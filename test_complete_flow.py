#!/usr/bin/env python3
"""
æµ‹è¯•å®Œæ•´çš„LLMå®ä¾‹åˆ›å»ºå’Œç”¨æˆ·é…ç½®æµç¨‹
"""
import os
import sys
import django
from django.conf import settings

# è®¾ç½®Djangoç¯å¢ƒ
os.environ.setdefault('DJANGO_SETTINGS_MODULE', 'EasyRAG.settings')
django.setup()

from rest_framework.test import APIRequestFactory
from rest_framework import serializers
from EasyRAG.llm_app.models import LLMTemplate, LLMInstance, LLMInstanceLLMModel, LLMModelUserConfig
from EasyRAG.user_app.models import User
from EasyRAG.llm_app.serializers import LLMInstanceSerializer, LLMModelUserConfigSerializer
from EasyRAG.llm_app.viewmodel import LLMInstanceViewModel, LLMModelUserConfigViewModel
import logging

logger = logging.getLogger(__name__)

def test_complete_flow():
    """æµ‹è¯•å®Œæ•´çš„LLMå®ä¾‹åˆ›å»ºå’Œç”¨æˆ·é…ç½®æµç¨‹"""
    print("å¼€å§‹æµ‹è¯•å®Œæ•´æµç¨‹...")
    
    # è·å–æˆ–åˆ›å»ºæµ‹è¯•ç”¨æˆ·
    user, created = User.objects.get_or_create(
        username='testuser',
        defaults={'email': 'test@example.com'}
    )
    if created:
        print(f"âœ… åˆ›å»ºæµ‹è¯•ç”¨æˆ·: {user.username}")
    else:
        print(f"âœ… ä½¿ç”¨ç°æœ‰æµ‹è¯•ç”¨æˆ·: {user.username}")
    
    # ä½¿ç”¨ç°æœ‰çš„æœ‰æ•ˆæ¨¡æ¿
    template = LLMTemplate.objects.filter(llm_template_id__isnull=False).exclude(llm_template_id='').first()
    if not template:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡æ¿ï¼Œè·³è¿‡æµ‹è¯•")
        return False
    
    print(f"âœ… ä½¿ç”¨ç°æœ‰æµ‹è¯•æ¨¡æ¿: {template.template_name} (ID: {template.llm_template_id})")
    
    # åˆ›å»ºè¯·æ±‚å·¥å‚
    factory = APIRequestFactory()
    request = factory.post('/api/llm/llm-instances/')
    request.user = user
    
    # å‡†å¤‡æµ‹è¯•æ•°æ®
    test_data = {
        'llm_template_id': template.llm_template_id,
        'llm_config': {
            'url': 'https://api.test.com/v1',
            'api_key': 'test_api_key_123'
        }
    }
    
    # æµ‹è¯•åºåˆ—åŒ–å™¨éªŒè¯
    serializer = LLMInstanceSerializer(data=test_data, context={'request': request})
    if not serializer.is_valid():
        print(f"âŒ åºåˆ—åŒ–å™¨éªŒè¯å¤±è´¥: {serializer.errors}")
        return False
    print("âœ… åºåˆ—åŒ–å™¨éªŒè¯é€šè¿‡")
    
    # æµ‹è¯•viewmodelï¼ˆè·³è¿‡è¿æ¥éªŒè¯ï¼‰
    view_model = LLMInstanceViewModel()
    
    # æ¨¡æ‹Ÿè¿æ¥éªŒè¯æˆåŠŸ
    def mock_get_llm_models(config, template_name):
        return [
            {'id': 'test-model-1', 'object': 'model'},
            {'id': 'test-model-2', 'object': 'model'}
        ]
    
    # ä¸´æ—¶æ›¿æ¢æ–¹æ³•
    original_method = view_model._get_llm_models
    view_model._get_llm_models = mock_get_llm_models
    
    try:
        # æ‰§è¡Œåˆ›å»º
        instance = view_model.perform_create(serializer)
        print(f"âœ… LLMå®ä¾‹åˆ›å»ºæˆåŠŸ: {instance.llm_instance_id}")
        
        # éªŒè¯LLMæ¨¡å‹æ˜¯å¦ä¿å­˜
        llm_models = LLMInstanceLLMModel.objects.filter(llm_instance=instance)
        print(f"âœ… ä¿å­˜äº† {llm_models.count()} ä¸ªLLMæ¨¡å‹")
        
        # æµ‹è¯•ç”¨æˆ·é…ç½®åˆ›å»º
        user_config_data = {
            'llm_instance_id': instance.llm_instance_id,
            'llm_model_id': 'test-model-1'
        }
        
        user_config_serializer = LLMModelUserConfigSerializer(
            data=user_config_data, 
            context={'request': request}
        )
        
        if not user_config_serializer.is_valid():
            print(f"âŒ ç”¨æˆ·é…ç½®åºåˆ—åŒ–å™¨éªŒè¯å¤±è´¥: {user_config_serializer.errors}")
            return False
        print("âœ… ç”¨æˆ·é…ç½®åºåˆ—åŒ–å™¨éªŒè¯é€šè¿‡")
        
        # æµ‹è¯•ç”¨æˆ·é…ç½®viewmodel
        user_config_view_model = LLMModelUserConfigViewModel()
        result = user_config_view_model.perform_create_after_delete(
            instance, 'test-model-1', user
        )
        
        if result:
            print("âœ… ç”¨æˆ·é…ç½®åˆ›å»ºæˆåŠŸ")
            
            # éªŒè¯ç”¨æˆ·é…ç½®æ˜¯å¦ä¿å­˜
            user_configs = LLMModelUserConfig.objects.filter(owner=user)
            print(f"âœ… ä¿å­˜äº† {user_configs.count()} ä¸ªç”¨æˆ·é…ç½®")
            
            # æ¸…ç†æµ‹è¯•æ•°æ®
            user_configs.delete()
            llm_models.delete()
            instance.delete()
            print("âœ… æµ‹è¯•æ•°æ®æ¸…ç†å®Œæˆ")
            
            return True
        else:
            print("âŒ ç”¨æˆ·é…ç½®åˆ›å»ºå¤±è´¥")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return False
    finally:
        # æ¢å¤åŸå§‹æ–¹æ³•
        view_model._get_llm_models = original_method

if __name__ == "__main__":
    success = test_complete_flow()
    if success:
        print("\nğŸ‰ å®Œæ•´æµç¨‹æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("\nâš ï¸ å®Œæ•´æµç¨‹æµ‹è¯•å¤±è´¥") 